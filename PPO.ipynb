{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4b3b384",
   "metadata": {},
   "source": [
    "# PPO（Proximal Policy Optimization，临近策略优化）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1c2169",
   "metadata": {},
   "source": [
    "## 前置概念 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3145b8bf",
   "metadata": {},
   "source": [
    "### Off Policy\n",
    "\n",
    "在 On Policy 中，采集数据使用的模型（策略）和训练的模型是同一个，每次训练完后都需要重新采集数据，然后重新训练。训练速度很慢。\n",
    "\n",
    "如果数据采集模型和训练模型不是同一个，并且采集的数据可以被多次用来训练，便可以提升采集速度，称为 Off Policy。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c3aa4b",
   "metadata": {},
   "source": [
    "![PPO-1](./images/PPO-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82454d6",
   "metadata": {},
   "source": [
    "例如：\n",
    "\n",
    "老师针对小明的表现表扬或者批评小明，如果老师表扬小明，那么小明加强受表扬的行为，反之亦然。这个过程可以视为一个 On Policy。\n",
    "\n",
    "其他同学根据老师表扬或批评小明所做的行为，而调整自己的行为，这就是 Off Policy。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a028f3",
   "metadata": {},
   "source": [
    "### 重要性采样"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1434b2fe",
   "metadata": {},
   "source": [
    "在 x 服从分布 p 的情况下，求 f(x) 的期望，公式为： \n",
    "\n",
    "$ E(f(x))_{x \\sim p(x)} = \\sum_{x} f(x) * p(x) \\approx \\frac{1}{N} \\sum_{n=1}^{N}f(x)_{x \\sim p(x)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83f770f",
   "metadata": {},
   "source": [
    "接下来，乘以并除以 x 在分布 q 下的概率 q(x):\n",
    "\n",
    "$ = \\sum_{x} f(x) * p(x) \\frac {q(x)} {q(x)} $\n",
    "\n",
    "$ = \\sum_{x} f(x) \\frac {p(x)} {q(x)} * q(x)  $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9181a7b9",
   "metadata": {},
   "source": [
    "最后的这个式子，可以看成在 x 服从分布 q 的情况下，求$ f(x) \\frac {p(x)} {q(x)}$的期望：\n",
    "\n",
    "$ = E(f(x) \\frac {p(x)} {q(x)})_{x \\sim q(x)} $ \n",
    "\n",
    "这样，在 q 分布下采样，计算$ f(x) \\frac {p(x)} {q(x)}$ 的期望就等于在 p 分布下采样，计算 $f(x)$ 的期望。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b92f0c4",
   "metadata": {},
   "source": [
    "再进一步，该式还约等于：\n",
    "\n",
    "$ \\approx \\frac{1}{N} \\sum_{n=1}^{N}f(x) \\frac{p(x)}{q(x)}_{x \\sim q(x) }  =   \\frac{1}{N} \\sum_{n=1}^{N}f(x)_{x \\sim p(x)} $\n",
    "\n",
    "表示为在 x 服从 q 分布下采样 N 条记录后，求均值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef7c213",
   "metadata": {},
   "source": [
    "使用重要性采样更新目标函数的梯度公式：\n",
    "\n",
    "$\\frac{1}{N}\\sum_{n=1}^{N}\\sum_{t=1}^{T_n} A_{\\theta }^{GAE}(s_{n}^{t},a_{n}^{t}) \\nabla \\log P_{\\theta }(a_{n}^{t}|s_{n}^{t}) $\n",
    "\n",
    "$ = \\frac{1}{N}\\sum_{n=1}^{N}\\sum_{t=1}^{T_n} A_{\\theta' }^{GAE}(s_{n}^{t},a_{n}^{t}) \\frac{P_{\\theta }(a_{n}^{t}|s_{n}^{t})}{P_{\\theta '}(a_{n}^{t}|s_{n}^{t})}  \\nabla \\log P_{\\theta }(a_{n}^{t}|s_{n}^{t})  $\n",
    "\n",
    "使用参考策略 $\\theta’$ 进行采样，计算其优势函数 $A_{\\theta' }^{GAE}(s_{n}^{t},a_{n}^{t})$，用他来更新训练策略 $\\theta$。\n",
    "\n",
    "这就将 On Policy 训练 转换为 Off Policy 训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b17b06",
   "metadata": {},
   "source": [
    "例如，$\\theta’$ 是小明的策略，$\\theta$ 是其他学生的策略。$\\theta’$ 的优势函数 $A_{\\theta' }^{GAE}(s_{n}^{t},a_{n}^{t})$ 就是老师对小明的批评或者表扬。\n",
    "\n",
    "你不能直接使用老师对小明的批评或者表扬来更新自己的行为。\n",
    "\n",
    "假如小明玩手机概率为0.2，你玩手机概率为0.4，在小明受到老师批评后，你不能只下降和小明一样的强度，而应该是小明的两倍才对。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24e0e11",
   "metadata": {},
   "source": [
    "继续推导公式，对数函数的求导公式为：$ \\nabla \\log f(x) = \\frac{\\nabla f(x)}{f(x)}  $\n",
    "\n",
    "带入表达式得：\n",
    "\n",
    "$ = \\frac{1}{N}\\sum_{n=1}^{N}\\sum_{t=1}^{T_n} A_{\\theta' }^{GAE}(s_{n}^{t},a_{n}^{t}) \\frac{P_{\\theta }(a_{n}^{t}|s_{n}^{t})}{P_{\\theta '}(a_{n}^{t}|s_{n}^{t})}  \\frac{\\nabla P_{\\theta }(a_{n}^{t}|s_{n}^{t})}{P_{\\theta }(a_{n}^{t}|s_{n}^{t})} $\n",
    "\n",
    "$ = \\frac{1}{N}\\sum_{n=1}^{N}\\sum_{t=1}^{T_n} A_{\\theta' }^{GAE}(s_{n}^{t},a_{n}^{t}) \\frac{\\nabla P_{\\theta }(a_{n}^{t}|s_{n}^{t})}{P_{\\theta '}(a_{n}^{t}|s_{n}^{t})} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c8b3e0",
   "metadata": {},
   "source": [
    "将该梯度公式去掉求梯度符号，再加上负号，就得到了 PPO 的损失函数：\n",
    "$$ \n",
    "Loss= - \\frac{1}{N}\\sum_{n=1}^{N}\\sum_{t=1}^{T_n} A_{\\theta' }^{GAE}(s_{n}^{t},a_{n}^{t}) \\frac{ P_{\\theta }(a_{n}^{t}|s_{n}^{t})}{P_{\\theta '}(a_{n}^{t}|s_{n}^{t})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6242e14",
   "metadata": {},
   "source": [
    "从公式可以看出，PPO 使用参考的策略 $\\theta'$ 来进行数据采样，计算优势函数 $A_{\\theta' }^{GAE}(s_{n}^{t},a_{n}^{t})$。\n",
    "\n",
    "用训练策略 $\\theta$ 做某个动作的概率 除以 参考策略 $\\theta'$ 做某个动作的概率来调整优势函数的度，得到运来训练 $\\theta$ 网络的损失函数。\n",
    "\n",
    "用参考策略进行数据采样，采样的数据多次训练 $\\theta$ 网络，这就解决了 On Policy 训练效率太低的问题。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40de2395",
   "metadata": {},
   "source": [
    "需要注意的是，参考策略 $\\theta'$ 不应该和训练策略 $\\theta$ 在同一情况下，给出各种动作的概率的分布过大。\n",
    "\n",
    "例如，如果老师对和你差不多的学生进行的表扬或批评，对你来说是由借鉴意义的。如果老师表扬或批评的对象和你差距过大，那就不具备参考意义了。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426c3d44",
   "metadata": {},
   "source": [
    "为此，我们引入 KL 散度（Kullback-Leibler divergence）来约束 训练策略 $\\theta$ 与参考策略 $\\theta'$ 的动作的概率分布的相似度。\n",
    "\n",
    "两个策略的动作的概率分布完全一样，KL 散度为 0。分布越不一致，KL 散度就越大。通过 $\\beta$ 来控制 KL 散度的大小。\n",
    "\n",
    "公式为：\n",
    "\n",
    "$$\n",
    "Loss_{ppo}= - \\frac{1}{N}\\sum_{n=1}^{N}\\sum_{t=1}^{T_n} A_{\\theta' }^{GAE}(s_{n}^{t},a_{n}^{t}) \\frac{P_{\\theta }(a_{n}^{t}|s_{n}^{t})}{P_{\\theta '}(a_{n}^{t}|s_{n}^{t})} + \\beta KL(P_{\\theta, \\theta'})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091320c7",
   "metadata": {},
   "source": [
    "还有一种方法是通过截断函数来替代 KL 散度，防止训练的策略和参考策略偏差过大。\n",
    "\n",
    "公式为：\n",
    "$$\n",
    "Loss_{ppo2} = - \\frac{1}{N}\\sum_{n=1}^{N}\\sum_{t=1}^{T_n} min( A_{\\theta' }^{GAE}(s_{n}^{t},a_{n}^{t}) \\frac{ P_{\\theta }(a_{n}^{t}|s_{n}^{t})}{P_{\\theta '}(a_{n}^{t}|s_{n}^{t})}, clip(\\frac{ P_{\\theta }(a_{n}^{t}|s_{n}^{t})}{P_{\\theta '}(a_{n}^{t}|s_{n}^{t})}), 1-\\varepsilon, 1+\\varepsilon )A_{\\theta' }^{GAE}(s_{n}^{t},a_{n}^{t})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c583f7",
   "metadata": {},
   "source": [
    "min（取较小值）中，左侧为原始公式，右侧为一个截断函数。\n",
    "\n",
    "截断函数有三个部分，如果$ \\frac{ P_{\\theta }(a_{n}^{t}|s_{n}^{t})}{P_{\\theta '}(a_{n}^{t}|s_{n}^{t})} $在 $[1-\\varepsilon, 1+\\varepsilon]$ 之间，就返回其本身的值，如果小于 $1-\\varepsilon$，就返回 $1-\\varepsilon$，如果大于 $1+\\varepsilon$，就返回 $1+\\varepsilon$。\n",
    "\n",
    "这就限制了 $ \\frac{ P_{\\theta }(a_{n}^{t}|s_{n}^{t})}{P_{\\theta '}(a_{n}^{t}|s_{n}^{t})} $ 中二者的不能相差太大。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409264d5",
   "metadata": {},
   "source": [
    "例如，假设 $ \\varepsilon = 0.2 $，如果奖励函数为正值，那么我们需要调整 $ P_{\\theta} $，使其尽可能的大，但他和 $ P_{\\theta'} $ 的比值不能超过 1.2。\n",
    "\n",
    "如果奖励函数为负值，那么我们需要调整 $ P_{\\theta} $，使其尽可能的小，但他和 $ P_{\\theta'} $ 的比值不能小于 0.8。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85774baa",
   "metadata": {},
   "source": [
    "# 参考\n",
    "- https://www.bilibili.com/video/BV1iz421h7gb/?spm_id_from=333.337.search-card.all.click&vd_source=8fb10652f8c3316e5308e66bcf6011f0\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
